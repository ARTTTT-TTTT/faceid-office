import os
import uuid
import faiss
import torch
import numpy as np
import pillow_heif
from PIL import Image
from tqdm import tqdm
from ultralytics import YOLO
from facenet_pytorch import InceptionResnetV1


from langchain_community.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain_community.docstore.in_memory import InMemoryDocstore


from app.utils.transform_factory import face_transform
from app.core.dummy_embedding import DummyEmbeddings
from app.constants.core_config import CoreConfig


pillow_heif.register_heif_opener()

# !FEATURE ‡πÅ‡∏¢‡∏Å vector ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ admin
"""
vector
‚îú‚îÄ‚îÄ [admin_id]/
‚îÇ ‚îú‚îÄ‚îÄ index.faiss
| ‚îî‚îÄ‚îÄindex.pkl
‚îú‚îÄ‚îÄ [admin_id]/
| ‚îú‚îÄ‚îÄ index.faiss
| ‚îî‚îÄ‚îÄ index.pkl
‚îî‚îÄ‚îÄ ...
"""

# !FEATURE HANDLE ERROR AND RETURN

"""
storage
‚îú‚îÄ‚îÄ face-images/
‚îÇ ‚îú‚îÄ‚îÄ [person_id]/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ uuid1.jpg
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ uuid2.jpg
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ uuid3.jpg
‚îÇ ‚îú‚îÄ‚îÄ [person_id]/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ uuid1.jpg
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ uuid2.jpg
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ uuid3.jpg
‚îÇ ‚îî‚îÄ‚îÄ ...
"""


class Vector:
    def __init__(self, device=None):
        self.config = CoreConfig()
        self.device = device or self.config.default_device
        self.embedding_dim = self.config.embedding_dim
        self.model = (
            InceptionResnetV1(pretrained=self.config.face_embedder_model)
            .eval()
            .to(self.device)
        )
        self.model_YOLO = YOLO(self.config.yolo_model_path)
        self.vector_path = self.config.vector_path
        self.face_images_path = self.config.face_images_path
        self.batch_size = self.config.batch_size

        self.transform = face_transform()

    def extract_face_vectors(self, face_images_folder: str):
        """
        Extract vectors from all person_id folders in the face_images_folder.
        """
        all_vectors = []
        all_docs = []

        for person_id_folder in tqdm(
            os.listdir(face_images_folder), desc=f"Processing {face_images_folder}"
        ):
            person_folder = os.path.join(face_images_folder, person_id_folder)
            if not os.path.isdir(person_folder):
                continue

            vectors, docs = self.extract_face_vectors_single(person_folder)
            all_vectors.extend(vectors)
            all_docs.extend(docs)

        return all_vectors, all_docs

    def extract_face_vectors_single(self, person_folder: str):
        """
        Extract vectors from all images inside a single person_folder.
        """
        vectors = []
        docs = []
        person_id = os.path.basename(person_folder)

        for img_file in os.listdir(person_folder):
            img_path = os.path.join(person_folder, img_file)

            try:
                img = Image.open(img_path).convert("RGB")
            except Exception as e:
                print(f"[!] Error loading image: {img_path} | {e}")
                continue

            # YOLO face detection
            results = self.model_YOLO.predict(source=img, conf=0.8, verbose=False)
            detections = results[0]

            if not detections.boxes or len(detections.boxes) == 0:
                print(f"[!] No face found in: {img_path}")
                continue

            # Crop face
            x1, y1, x2, y2 = map(int, detections.boxes[0].xyxy[0])
            cropped = img.crop((x1, y1, x2, y2))

            # Convert to tensor
            face_tensor = self.transform(cropped).unsqueeze(0).to(self.device)

            # Extract embedding
            with torch.no_grad():
                embedding = self.model(face_tensor).squeeze().cpu().numpy()
                embedding = embedding / np.linalg.norm(embedding)

                vectors.append(embedding)
                docs.append(
                    Document(
                        page_content="face_vector",
                        metadata={"name": person_id, "image": img_file},
                    )
                )

        return vectors, docs

    """
    ========== CRUD ===========
    """

    def get_people_vectors(self):
        """
        ‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞ metadata ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏†‡∏≤‡∏û
        """
        db = FAISS.load_local(
            self.vector_path,
            embeddings=DummyEmbeddings(),
            allow_dangerous_deserialization=True,
        )

        people_data = {}

        for idx, doc_id in db.index_to_docstore_id.items():
            doc = db.docstore.search(doc_id)
            name = doc.metadata.get("name")

            if name not in people_data:
                people_data[name] = []

            result_info = {"index": idx, "doc_id": doc_id, "metadata": doc.metadata}
            people_data[name].append(result_info)

        # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
        for person_name, vectors in people_data.items():
            print(f"\nüë§ {person_name} - {len(vectors)} vectors")
            for vector in vectors:
                print(
                    f"   ‚Ü≥ Index={vector['index']}, Doc ID='{vector['doc_id']}', Metadata={vector['metadata']}"
                )

        return people_data

    def get_person_vectors(self, person_id: str):
        """
        database ‡∏°‡∏µ person_name(‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡∏µ‡πà‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á)
        """
        db = FAISS.load_local(
            self.vector_path,
            embeddings=DummyEmbeddings(),
            allow_dangerous_deserialization=True,
        )
        results = []
        for idx, doc_id in db.index_to_docstore_id.items():
            doc = db.docstore.search(doc_id)
            if doc.metadata.get("name") == person_id:
                result_info = {"index": idx, "doc_id": doc_id, "metadata": doc.metadata}
                results.append(result_info)
                print(
                    f"[üîç] Found vector: Index={idx}, Doc ID='{doc_id}', Metadata={doc.metadata}"
                )
        print(f"[‚ÑπÔ∏è] Total vectors found for '{person_id}': {len(results)}")
        return results

    def get_total_vectors(self):
        """
        ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô FAISS database
        """
        db = FAISS.load_local(
            self.vector_path,
            embeddings=DummyEmbeddings(),
            allow_dangerous_deserialization=True,
        )
        total_count = db.index.ntotal
        print(f"[üìä] Total face vectors in FAISS: {total_count}")
        return total_count

    def build_empty_vectors(self):
        """
        index    : ‡∏™‡∏£‡πâ‡∏≤‡∏á vector ‡∏Ç‡∏ô‡∏≤‡∏î 512
        docstore : ‡∏™‡∏£‡πâ‡∏≤‡∏á ‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á ‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö metadata ‡πÄ‡∏ä‡πà‡∏ô ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
        index_to_docstore_id : dictionary mapping ‡∏à‡∏≤‡∏Å index ‡πÉ‡∏ô FAISS ‡πÑ‡∏õ‡∏¢‡∏±‡∏á ID ‡πÉ‡∏ô docstore

        db
        index: FAISS index ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå
        docstore: ‡πÄ‡∏Å‡πá‡∏ö metadata ‡πÄ‡∏ä‡πà‡∏ô ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏π‡∏õ
        index_to_docstore_id: mapping ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á index ‡∏Å‡∏±‡∏ö ID ‡πÉ‡∏ô docstore
        """

        index = faiss.IndexIDMap(faiss.IndexFlatL2(self.embedding_dim))  # ‡πÉ‡∏ä‡πâ IndexIDMap
        docstore = InMemoryDocstore({})
        index_to_docstore_id = {}
        db = FAISS(
            embedding_function=DummyEmbeddings(),
            index=index,
            docstore=docstore,
            index_to_docstore_id=index_to_docstore_id,
        )
        db.save_local(self.vector_path)
        print("[‚úÖ] Empty FAISS database created and saved.")

    def build_vectors(self):
        """
        batch_size : ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô batch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ RAM
        index : ‡∏™‡∏£‡πâ‡∏≤‡∏á IndexFlatL2 -‡∏Ç‡∏ô‡∏≤‡∏î 512
        index.add : ‡πÄ‡∏≠‡∏≤ vector ‡∏ó‡∏µ‡πà extract ‡∏°‡∏≤‡πÑ‡∏õ‡πÉ‡∏™‡πà ‡πÉ‡∏ô index
        docstore : loop ‡πÄ‡∏≠‡∏≤ vector ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô index ‡∏°‡∏≤‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô docstore

        doc_ids : ‡πÄ‡∏Å‡πá‡∏ö uuid ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô vector ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å image_person
        docstore_dict : ‡πÄ‡∏õ‡πá‡∏ô unique id ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏†‡∏≤‡∏û
        index_to_docstore_id : ‡∏™‡∏™‡∏£‡πâ‡∏≤‡∏á dictionary ‡∏ó‡∏µ‡πà mapping ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á ‡∏•‡∏≥‡∏î‡∏±‡∏ö index ‡πÉ‡∏ô FAISS ‡∏Å‡∏±‡∏ö ID ‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏ô docstore
        """

        vectors, docs = self.extract_face_vectors(self.face_images_path)
        if not vectors:
            print("[‚ùó] No face vectors extracted.")
            return

        index = faiss.IndexIDMap(faiss.IndexFlatL2(self.embedding_dim))
        ids = np.array(range(len(vectors)), dtype=np.int64)
        for i in range(0, len(vectors), self.batch_size):
            batch_vecs = np.array(vectors[i : i + self.batch_size]).astype(np.float32)
            batch_ids = ids[i : i + self.batch_size]
            index.add_with_ids(batch_vecs, batch_ids)

        docstore_dict = {}
        index_to_docstore_id = {}
        for i, doc in enumerate(docs):
            doc_id = str(uuid.uuid4())
            docstore_dict[doc_id] = doc
            index_to_docstore_id[i] = doc_id

        db = FAISS(
            embedding_function=DummyEmbeddings(),
            index=index,
            docstore=InMemoryDocstore(docstore_dict),
            index_to_docstore_id=index_to_docstore_id,
        )
        db.save_local(self.vector_path)
        print("[‚úÖ] FAISS database built with IndexIDMap and saved.")
        return True

    def update_person_vectors(self, person_id: str):
        """
        db
        load model vector : ‡πÄ‡∏î‡∏¥‡∏°‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô base

        existing_keys : ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ Metadata={'name': 'Pun', 'image': 'IMG_6371.HEIC'}
        ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô model ‡∏°‡∏±‡πâ‡∏¢

        """
        person_folder = os.path.join(self.config.face_images_path + "/" + person_id)

        db = FAISS.load_local(
            self.vector_path,
            embeddings=DummyEmbeddings(),
            allow_dangerous_deserialization=True,
        )
        existing_keys = {
            f"{doc.metadata['name']}_{doc.metadata['image']}"
            for doc in db.docstore._dict.values()
        }

        new_vectors, new_docs = self.extract_face_vectors_single(person_folder)
        if not new_vectors:
            
            print("[‚ùó] No new faces to add.")
            return

        filtered_vectors = []
        filtered_docs = []
        for vec, doc in zip(new_vectors, new_docs):
            key = f"{doc.metadata['name']}_{doc.metadata['image']}"
            if key not in existing_keys:
                filtered_vectors.append(vec)
                filtered_docs.append(doc)
                existing_keys.add(key)

        if not filtered_vectors:
            print("[‚ÑπÔ∏è] All vectors already exist in FAISS.")
            return

        current_count = db.index.ntotal
        ids = np.array(
            range(current_count, current_count + len(filtered_vectors)), dtype=np.int64
        )
        db.index.add_with_ids(np.array(filtered_vectors).astype(np.float32), ids)

        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï docstore ‡πÅ‡∏•‡∏∞ index_to_docstore_id
        for i, (vec, doc) in enumerate(zip(filtered_vectors, filtered_docs)):
            doc_id = str(uuid.uuid4())
            db.docstore._dict[doc_id] = doc
            db.index_to_docstore_id[current_count + i] = doc_id

        db.save_local(self.vector_path)

    def delete_person_vectors(self, person_id: str):
        """
        ‡πÄ‡∏ä‡πá‡∏Ñ‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠ ‡∏ß‡πà‡∏≤‡πÉ‡∏ô metadata ‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏±‡πâ‡∏¢ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡∏•‡∏ö
        """
        # !FEATURE ‡∏¢‡∏¥‡∏á API ‡∏•‡∏ö person_folder

        db = FAISS.load_local(
            self.vector_path,
            embeddings=DummyEmbeddings(),
            allow_dangerous_deserialization=True,
        )

        indices_to_delete = []
        for idx, doc_id in db.index_to_docstore_id.items():
            doc = db.docstore.search(doc_id)
            if doc.metadata.get("name") == person_id:
                faiss_id = idx
                indices_to_delete.append(faiss_id)

        if not indices_to_delete:
            print(f"[‚ùó] No vectors found with name '{person_id}'.")
            return

        print(f"[üóëÔ∏è] Deleting {len(indices_to_delete)} vectors for '{person_id}'...")
        db.index.remove_ids(np.array(indices_to_delete, dtype=np.int64))

        remaining_index_map = {}
        remaining_docstore = {}
        for idx, doc_id in db.index_to_docstore_id.items():
            if idx not in indices_to_delete:
                remaining_index_map[idx] = doc_id
                remaining_docstore[doc_id] = db.docstore.search(doc_id)

        db.docstore = InMemoryDocstore(remaining_docstore)
        db.index_to_docstore_id = remaining_index_map

        db.save_local(self.vector_path)
