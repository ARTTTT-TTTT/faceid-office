import os
import cv2
import faiss
import numpy as np

from PIL import Image
from ultralytics import YOLO
from collections import Counter
from torchvision import transforms
from langchain.vectorstores import FAISS
from facenet_pytorch import InceptionResnetV1
from langchain.embeddings.base import Embeddings

"""
‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° (environment variable) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö OpenMP library ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏Ç‡∏ô‡∏≤‡∏ô 
‡πÄ‡∏ä‡πà‡∏ô NumPy, PyTorch, TensorFlow, ‡∏´‡∏£‡∏∑‡∏≠ scikit-learn ‡∏ö‡∏ô‡∏ö‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö (‡πÄ‡∏ä‡πà‡∏ô macOS ‡∏´‡∏£‡∏∑‡∏≠ Windows)


üí• ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏Å‡πÄ‡∏Å‡∏¥‡∏î:
‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≤‡∏à‡πÄ‡∏à‡∏≠ error ‡πÄ‡∏ä‡πà‡∏ô‡∏ô‡∏µ‡πâ:

OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.
‡∏´‡∏£‡∏∑‡∏≠

OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program.
‡∏ã‡∏∂‡πà‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ OpenMP (‡∏ï‡∏±‡∏ß‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ multi-threading) ‡∏ñ‡∏π‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏ã‡πâ‡∏≥‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏ó‡∏µ‡πà ‡πÄ‡∏ä‡πà‡∏ô‡∏à‡∏≤‡∏Å PyTorch ‡πÅ‡∏•‡∏∞ NumPy ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô

‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ KMP_DUPLICATE_LIB_OK = "TRUE"
‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE" ‡∏à‡∏∞:

‡∏ö‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö ‡∏¢‡∏≠‡∏°‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ OpenMP ‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ

‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á crash ‡∏´‡∏£‡∏∑‡∏≠ error ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ library
"""

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


class FaceBlob:
    def __init__(self, id, position, image=None):
        """
        blob_id , id : (‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠),
        position     : (center = x,y),
        image        : (‡∏†‡∏≤‡∏û‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å update ‡∏°‡∏≤),
        life         : (‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô 5 frame ‡∏à‡∏∞ ‡∏•‡∏ö blob ‡∏ô‡∏µ‡πâ‡∏ó‡∏¥‡πâ‡∏á),

        matched_person_name = None
        (‡∏ñ‡πâ‡∏≤ blob ‡∏ô‡∏µ‡πâ matched ‡∏Å‡∏±‡∏ö ‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏Ñ‡∏£‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏∞‡πÅ‡∏õ‡∏•‡∏á ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà matched = unknown)

        match_history : ‡πÄ‡∏Å‡πá‡∏ö‡∏ß‡πà‡∏≤ blob ‡∏ô‡∏µ‡πâ ‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏à‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏Ñ‡∏£‡∏ö‡πâ‡∏≤‡∏á {"name",‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠}

        kalman : setup kalmanfilter ‡∏ï‡πà‡∏≤‡∏á‡πÜ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ predict
        kalman.processNoiseCov *0.03 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£ predict ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏ï‡πâ‡∏≠‡∏á add noise
        """
        self.id = id
        self.position = position
        self.image = image
        self.life = DetectionProcessingService.LIFE_TIME
        self.matched_person_name = None
        self.match_history = {}
        # Kalman Filter setup for tracking x, y
        self.kalman = cv2.KalmanFilter(4, 2)  # state: [x, y, dx, dy]
        self.kalman.measurementMatrix = np.array(
            [[1, 0, 0, 0], [0, 1, 0, 0]], np.float32
        )
        self.kalman.transitionMatrix = np.array(
            [[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32
        )
        self.kalman.processNoiseCov = np.eye(4, dtype=np.float32) * 0.03
        initial_state = np.array(
            [[position[0]], [position[1]], [0], [0]], dtype=np.float32
        )
        self.kalman.statePre = initial_state.copy()
        self.kalman.statePost = initial_state.copy()

    def predict_position(self):
        prediction = self.kalman.predict()
        return int(prediction[0, 0]), int(prediction[1, 0])

    def update(self, position, image, matched_person_name):
        """
        update        :Blob status ‡πÉ‡∏ô‡∏Å‡πá‡∏ì‡∏µ‡∏ó‡∏µ‡πà matched ‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏Å‡πà‡∏≤
        match_history : ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ß‡πà‡∏≤ Blob ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏à‡∏≠‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ß‡πà‡∏≤‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏à‡∏≠ blob ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ ‡∏Å‡∏µ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß
        """
        self.position = position
        self.image = image
        self.life = DetectionProcessingService.LIFE_TIME

        self.kalman.correct(np.array([[position[0]], [position[1]]], dtype=np.float32))

        self.matched_person_name = matched_person_name
        self.match_history[matched_person_name] = (
            self.match_history.get(matched_person_name, 0) + 1
        )

    def get_match_summary(self):
        """
        SURE_KNOW = 5     : ‡∏°‡∏±‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡∏ô‡∏±‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ 5 frame
        SURE_UNKNOWN = 5   : ‡∏°‡∏±‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡πÉ‡∏Ñ‡∏£‡∏î‡πâ‡∏ß‡∏¢ 5 frame

        named_matches     : ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö vector
        valid_named       : ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏±‡∏ô‡πÉ‡∏à‡∏ß‡πà‡∏≤ SURE_KNOW

        best_match_name   : ‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πà‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        best_match_count  : ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏°‡∏µ‡∏Å‡∏µ‡πà‡∏Ñ‡∏ô
        unknown_count      : ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô get ‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠

        Output :
          ‡∏ñ‡πâ‡∏≤ unknown ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ = ‡∏Å‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 2 ‡πÄ‡∏ó‡πà‡∏≤ ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô unknown
          ‡∏ñ‡πâ‡∏≤ ‡∏ä‡∏∑‡πà‡∏≠ ‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ä‡∏∑‡πà‡∏≠ ‡∏Ñ‡∏ô‡∏ô‡∏±‡πâ‡∏ô
        """

        SURE_KNOW = 5
        SURE_UNKNOWN = 5
        summary = ", ".join(f"{k}: {v}" for k, v in self.match_history.items())
        # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà "Unknow" ‡πÅ‡∏•‡∏∞‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ï‡∏£‡∏á ‚â• SURE_KNOW
        valid_named = {
            name: count
            for name, count in self.match_history.items()
            if name != "Unknown" and count >= SURE_KNOW
        }

        best_match_name, best_match_count = (None, 0)
        if valid_named:
            best_match_name, best_match_count = max(
                valid_named.items(), key=lambda x: x[1]
            )

        unknown_count = self.match_history.get("Unknown", 0)

        if unknown_count >= max(SURE_UNKNOWN, best_match_count * 2):
            return "Unknown", summary, self.image

        if (
            best_match_name
            and unknown_count <= best_match_count * 2
            and unknown_count >= SURE_UNKNOWN
        ):
            return best_match_name, summary, self.image

        # ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÅ‡∏ô‡πà‡∏ä‡∏±‡∏î
        return None, None, None


class DummyEmbeddings(Embeddings):
    def embed_documents(self, texts):
        return texts

    def embed_query(self, text):
        return text


class DetectionProcessingService:
    # === Configure ===
    base_dir = os.getcwd()
    YOLO_MODEL = "yolov11n-face.pt"  # ‡∏ä‡∏∑‡πà‡∏≠ model YOLO
    FACE_EMBEDDER_MODEL = (
        "vggface2"  # ‡∏ä‡∏∑‡πà‡∏≠ model facenet ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á ‡∏Å‡∏±‡∏ö model ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡∏µ‡∏ö ‡∏≠‡∏±‡∏Å‡πÉ‡∏ô .faiss ‡∏î‡πâ‡∏ß‡∏¢
    )

    yolo_model_path = os.path.join(base_dir, "model", YOLO_MODEL)
    known_faces_path = os.path.join(base_dir, "data", "faiss_Store")
    index_path = os.path.join(known_faces_path, "index.faiss")
    model_YOLO = YOLO(yolo_model_path)
    model_Facenet = InceptionResnetV1(pretrained=FACE_EMBEDDER_MODEL).eval()

    # === Confident ===
    YOLO_THRESHOLD = 0.75  # 0.7 - 0.9 ‡∏Ñ‡∏∑‡∏≠ ‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏≤‡∏Å‡∏¢‡∏∑‡πà‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô
    FACENET_THRESHOLD = 0.65  # 0.6 - 0.8 ‡∏Ñ‡∏∑‡∏≠ ‡∏¢‡∏±‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏¢‡∏∑‡πà‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏¢‡∏¥‡πà‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô

    # === Blob Configure ===
    LIFE_TIME = 5  # ‡∏≠‡∏≤‡∏¢‡∏∏‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡πÇ‡∏î‡∏ô‡∏•‡∏ö maximum 5 frame ‡∏ñ‡πâ‡∏≤‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà
    ID_FACE = 0  # ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏à‡∏≠‡πÉ‡∏ô ‡∏£‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏ô‡∏µ‡πâ start ‡∏ó‡∏µ‡πà 0
    BLOBS = []  # ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô frame ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô

    transform = transforms.Compose(
        [
            transforms.Resize((160, 160)),
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),
        ]
    )

    # === Faiss Configure ===
    index = None
    index_ivf = None  # ‡πÄ‡∏ä‡πá‡∏Ñ dimension ‡∏Ç‡∏≠‡∏á ‡πÅ‡∏ï‡πà‡∏•‡∏∞ vector
    id_to_name = {}  # ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà match ‡∏ï‡∏≠‡∏ô‡∏´‡∏≤ near less neigbors

    @classmethod
    def load_faiss_index(cls):
        """
        cls.faiss_db :
          ‡πÉ‡∏ä‡πâ FAISS.load_local()           : ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏à‡∏≤‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå cls.FAISS_DIR
          DummyEmbeddings                 : ‡πÄ‡∏õ‡πá‡∏ô placeholder ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ embed ‡πÉ‡∏´‡∏°‡πà
          allow_dangerous_deserialization : ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î object ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ (‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡∏Å‡∏£‡∏ì‡∏µ)

        IndexIDMap : ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ vector ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏∏ ID ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ vector ‡πÑ‡∏î‡πâ

        """
        # ‡πÇ‡∏´‡∏•‡∏î FAISS ‡∏à‡∏≤‡∏Å langchain
        cls.faiss_db = FAISS.load_local(
            cls.known_faces_path,
            embeddings=DummyEmbeddings(),
            allow_dangerous_deserialization=True,
        )
        cls.index = faiss.read_index(cls.index_path)

        dimension = (
            cls.index.d
        )  # ‡∏™‡∏£‡πâ‡∏≤‡∏á Dimension ‡∏Ç‡∏≠‡∏á embedding vector ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡πÅ‡∏ö‡∏ö Euclidean
        new_index = faiss.IndexFlatL2(
            dimension
        )  # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ IndexFlatIP ‡∏´‡∏≤‡∏Å‡πÉ‡∏ä‡πâ inner product

        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô index ‡πÉ‡∏´‡∏°‡πà
        cls.index_ivf = faiss.IndexIDMap(new_index)
        vectors = cls.index.reconstruct_n(0, cls.index.ntotal)
        ids = np.array(range(cls.index.ntotal), dtype=np.int64)
        cls.index_ivf.add_with_ids(vectors, ids)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á dictionary mapping ID ‚Üí Name
        cls.id_to_name = {}
        docstore = cls.faiss_db.docstore._dict
        for idx, value in docstore.items():
            try:
                name = value.metadata.get("name", "Unknown")
                cls.id_to_name[int(idx)] = name
            except:
                continue

    @classmethod
    def detect_faces(cls, frame):
        """
        YOLO ‡∏´‡∏≤ x,y ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ô
        """
        results = cls.model_YOLO.predict(
            source=frame, conf=cls.YOLO_THRESHOLD, verbose=False
        )
        return results[0]

    @classmethod
    def extract_faces_and_positions(cls, frame, detections):
        """
        position : x,y ‡∏Ç‡∏ß‡∏≤‡∏ö‡∏ô ‡∏ã‡πâ‡∏≤‡∏¢‡∏•‡πà‡∏≤‡∏á ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô fram
        faces    : ‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà cropped ‡πÑ‡∏î‡πâ
        """

        positions, faces = [], []
        for box in detections.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            faces.append(frame[y1:y2, x1:x2])
            positions.append(((x1 + x2) // 2, (y1 + y2) // 2))
        return positions, faces

    @classmethod
    def image_embedding(cls, cropped_image):
        """
        pil_img   : ‡πÄ‡∏≠‡∏≤‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏°‡∏≤‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô rgb
        tensor    : ‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û rgb ‡πÄ‡∏≠‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ compose ‡πÅ‡∏•‡∏∞ ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á ‡∏Ñ‡πà‡∏≤ RGB ‡πÑ‡∏õ 0-1 ‡πÅ‡∏•‡∏∞ ‡∏ó‡∏≥ Normalize ‡πÅ‡∏•‡∏∞ .unsqueeze ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å ‚Äú‡∏†‡∏≤‡∏û‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‚Äù ‚Üí ‚Äúbatch ‡∏Ç‡∏ô‡∏≤‡∏î 1‚Äù

        cls.model_Facenet(tensor).detach().cpu().numpy()[0] => ‡πÄ‡∏Ç‡πâ‡∏≤ facenet ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏µ‡∏ö‡πÄ‡∏õ‡πá‡∏ô vector
        .detach() : ‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô inference ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö gradient
        .cpu()    : numpy() ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡∏ö‡∏ô CPU
        .numpy()  : PyTorch Tensor ‚Üí NumPy array
        [0]       : ‡∏î‡∏∂‡∏á batch ‡πÅ‡∏£‡∏Å‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ input ‡∏°‡∏µ shape [1, 512] ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ [512])
        """
        pil_img = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))
        tensor = cls.transform(pil_img).unsqueeze(0)
        return cls.model_Facenet(tensor).detach().cpu().numpy()[0]

    @classmethod
    def find_best_match(cls, embedding):
        """
        embedding     : Normalize ‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå (‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå = 1) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        distances     : ‡∏£‡∏∞‡∏¢‡∏∞‡∏´‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö (‡∏¢‡∏¥‡πà‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏¢‡∏¥‡πà‡∏á‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢)
        indices       : index ‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        K ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤ vector ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ ‡∏Å‡∏±‡∏ö vector ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏≤ ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏µ‡πà ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡∏Ç‡∏≠‡∏á vector 1,3,5,..
        (‡πÄ‡∏£‡∏≤‡πÉ‡∏™‡πà  ‡∏Ñ‡∏ô‡∏•‡∏∞ 3 vector ‡∏Å‡πá‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ 3 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏•‡πâ‡∏ô ‡∏ï‡πà‡∏≠‡∏Ñ‡∏ô ‡∏ñ‡πâ‡∏≤‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏π‡∏õ‡∏ï‡πà‡∏≠‡∏Ñ‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ)

        matched_names    : ‡πÄ‡∏≠‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô tuple ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô unknown
        name_counter     : ‡πÉ‡∏ô K(3) ‡∏Ñ‡∏ô ‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏Ñ‡∏£‡πÄ‡∏¢‡∏≠‡∏∞‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        most_common_name : ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠ vector ‡πÄ‡∏¢‡∏≠‡∏∞‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

        Output:
        ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô , ‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏ß "Unknown"
        """
        # Check ‡∏ß‡πà‡∏≤load ‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ô‡∏£‡∏∂‡∏¢‡∏±‡∏á
        if cls.index is None:
            print("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÇ‡∏´‡∏•‡∏î vector .faiss")
            raise ValueError("FAISS index not loaded.")

        embedding = embedding / np.linalg.norm(embedding)
        distances, indices = cls.index_ivf.search(np.array([embedding]), k=3)
        matched_names = [cls.id_to_name.get(int(i), "Unknown") for i in indices[0]]
        name_counter = Counter(matched_names)
        if not name_counter:
            return "Unknown"

        most_common_name = name_counter.most_common(1)[0][0]
        if distances[0][0] < cls.FACENET_THRESHOLD:
            return most_common_name
        else:
            return "Unknown"

    @classmethod
    def match_or_create_blob(cls, pos, face, person, matched_ids):
        """
        ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô blob ‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡πá‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ ‡πÉ‡∏Ñ‡∏£‡πÉ‡∏Å‡∏•‡πâ ‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏±‡∏ô‡πÑ‡∏´‡∏ô

        blob.predict_position() : ‡πÉ‡∏ä‡πâ kalman filter position ‡πÉ‡∏´‡∏°‡πà
        ‡∏ñ‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ‡∏à‡∏∞ update ‡∏Ñ‡πà‡∏≤ ‡πÉ‡∏´‡πâ blob

        cls.BLOBS : ‡πÄ‡∏Å‡πá‡∏ö status ‡∏ï‡πà‡∏≤‡∏á‡πÜ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô frame
        new_blob  : ‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ status ‡πÑ‡∏õ add new_blob

        """

        for blob in cls.BLOBS:
            if cls.is_near(blob.predict_position(), pos):
                blob.update(
                    position=pos,
                    image=face,
                    matched_person_name=person or blob.matched_person_name,
                )
                matched_ids.add(blob.id)
                return

        blob_id = f"face_{cls.ID_FACE}"
        cls.ID_FACE += 1
        new_blob = FaceBlob(id=blob_id, position=pos, image=face)
        new_blob.matched_person_name = person
        cls.BLOBS.append(new_blob)

    @staticmethod
    def is_near(pos1, pos2, threshold=250):
        """
        pos1 : ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤ "‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏" ‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏£‡∏á‡∏ô‡∏±‡πâ‡∏ô ‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏≠‡∏á Kalman Filter
        pos2 : ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÄ‡∏ü‡∏£‡∏°‡∏ô‡∏µ‡πâ
        """

        return (pos1[0] - pos2[0]) ** 2 + (pos1[1] - pos2[1]) ** 2 < threshold**2

    @classmethod
    def decrease_life_and_remove(cls, matched_ids=set()):
        """
        to_remove : list ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å ‡∏•‡∏ö ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ life_time < 5

        name    : ‡∏ä‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏á blob ‡∏Ñ‡∏ô‡∏ô‡∏µ‡πâ ‡∏ó‡∏µ‡πà matched ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        summary : blob ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡πÄ‡∏Ñ‡∏¢‡∏ñ‡∏π‡∏Å‡πÄ‡∏à‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏Ñ‡∏£‡∏ö‡πâ‡∏≤‡∏á
        img     : ‡∏†‡∏≤‡∏û‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å cap

        """
        to_remove = []
        for blob in cls.BLOBS:
            if blob.id not in matched_ids:
                blob.life -= 1
                if blob.life <= 0:
                    to_remove.append(blob)
        # blob ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å ‡∏•‡∏ö‡∏ó‡∏¥‡πâ‡∏á‡πÄ‡∏û‡∏£‡∏≤‡∏∞ life_time ‡∏´‡∏°‡∏î
        for blob in to_remove:
            name, summary, img = blob.get_match_summary()
            if img is not None:
                img = cv2.resize(img, (500, 500))
                if name == "Unknown":
                    cv2.imshow("Unknown", img)
                if name:
                    cv2.imshow(f"{name}", img)

            print(
                f"[REMOVE] {blob.id} ‚Üí Most likely matched: {name} Sumarize: {summary}]"
            )
            cls.BLOBS.remove(blob)

    @classmethod
    def tracking_face(cls, frame):
        """YOLO DETECTED FACE"""
        detections = cls.detect_faces(frame)
        annotated_frame = detections.plot()

        """‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ return ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡πÄ‡∏•‡∏¢"""
        if not detections.boxes:
            # ‡∏•‡∏î life_time ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô frame ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
            cls.decrease_life_and_remove()
            return annotated_frame, cls.BLOBS

        """
        ‡∏´‡∏≤‡πÄ‡∏à‡∏≠ ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏ö recognization
        input:
          frame     : frame ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏£‡∏±‡∏ö‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á
          detection : x,y ‡∏ó‡∏±‡πâ‡∏á‡∏°‡∏∏‡∏°‡∏Ç‡∏ß‡∏≤‡∏ö‡∏ô‡πÅ‡∏•‡∏∞‡∏ã‡πâ‡∏≤‡∏¢‡∏•‡πà‡∏≤‡∏á

        output:
          positions : x,y ‡∏Ç‡∏≠‡∏á center ‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠
          faces     : ‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠
        """
        positions, faces = cls.extract_faces_and_positions(frame, detections)

        matched = set()  # list ‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô frame
        """
        zip(positions, faces) : ‡πÄ‡∏≠‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• list position,faces ‡∏°‡∏≤‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß loop ‡∏≠‡∏≠‡∏Å‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
        """
        for pos, face in zip(positions, faces):
            embedding = cls.image_embedding(face)  # ‡∏ö‡∏µ‡∏ö faces(‡∏†‡∏≤‡∏û‡∏´‡∏ô‡πâ‡∏≤) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô vector
            person = cls.find_best_match(
                embedding
            )  # ‡πÄ‡∏≠‡∏≤‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ö‡∏µ‡∏ö‡πÑ‡∏î‡πâ‡πÑ‡∏õ check ‡∏ß‡πà‡∏≤ ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÉ‡∏Ñ‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
            cls.match_or_create_blob(pos, face, person, matched)

        cls.decrease_life_and_remove(matched_ids=matched)
        return annotated_frame, cls.BLOBS


if __name__ == "__main__":
    # load vector known faces form path:D:\Project\FaceDetect\data\faiss_Eng_Name
    DetectionProcessingService.load_faiss_index()

    # define input frame
    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # use DirectShow to avoid MSMF error
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 960)
    cap.set(cv2.CAP_PROP_FPS, 5)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        annotated_frame, blobs = DetectionProcessingService.tracking_face(frame)

        # label bounding box
        # Draw blob ID and matched name
        # for blob in blobs:
        #     x, y = blob.position
        #     text = f"{blob.id}: {blob.matched_person_name or 'Unknow'}"
        #     cv2.putText(
        #         annotated_frame,
        #         text,
        #         (x - 40, y - 10),
        #         cv2.FONT_HERSHEY_SIMPLEX,
        #         0.5,
        #         (0, 255, 0),
        #         2,
        #     )
        #     cv2.circle(annotated_frame, (x, y), 5, (255, 0, 0), -1)

        cv2.imshow("Face Tracking", annotated_frame)

        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    cap.release()
    cv2.destroyAllWindows()
